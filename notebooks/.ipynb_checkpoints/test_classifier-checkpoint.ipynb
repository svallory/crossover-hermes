{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for Testing Email Analyzer Agent\n",
    "\n",
    "This notebook allows for manual testing of the Email Analyzer agent (`classifier.py`). It performs the following steps:\n",
    "1.  Installs and imports necessary libraries.\n",
    "2.  Loads configuration and input data (products and emails) from the Google Spreadsheet provided in the assignment.\n",
    "3.  Initializes the Email Analyzer agent.\n",
    "4.  Processes each email using the agent to get its classification.\n",
    "5.  Formats the classification results.\n",
    "6.  Saves the results to a new Google Spreadsheet in the 'email-classification' sheet.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installations\n",
    "\n",
    "First, let's install the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas openai gspread gspread-dataframe google-auth google-auth-oauthlib google-auth-httplib2 pydantic langchain langchain-openai python-dotenv nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.colab import auth # For Colab authentication\n",
    "from google.auth import default # For Colab authentication\n",
    "from IPython.display import display\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add src directory to Python path to import custom modules\n",
    "# This assumes the notebook is in 'notebooks/' and 'src/' is at the root of the workspace\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Project-specific imports\n",
    "from src.config import HermesConfig\n",
    "from src.agents.classifier import analyze_email_node, EmailAnalysis\n",
    "from src.state import HermesState\n",
    "\n",
    "print(\"Setup complete. Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define constants for spreadsheet IDs, names, and initialize `HermesConfig`.\n",
    "Make sure you have a `.env` file in the root of your workspace with your `OPENAI_API_KEY` and `OPENAI_BASE_URL` if you are using the Crossover-provided key, or your `GEMINI_API_KEY` if using Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=os.path.join(module_path, '.env'), override=True)\n",
    "\n",
    "# Configuration for input and output\n",
    "INPUT_SPREADSHEET_ID = '14fKHsblfqZfWj3iAaM2oA51TlYfQlFT4WKo52fVaQ9U' # From assignment\n",
    "OUTPUT_SPREADSHEET_NAME = 'Hermes - Email Analyzer Test Output' # Name for the new spreadsheet\n",
    "\n",
    "# Initialize HermesConfig\n",
    "# Update llm_model_name, llm_api_key, llm_base_url as needed, or ensure they are in your .env\n",
    "hermes_config = HermesConfig(\n",
    "    llm_model_name=os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o\"), # or your preferred model\n",
    "    llm_api_key=os.getenv(\"OPENAI_API_KEY\"), # Loaded from .env\n",
    "    llm_base_url=os.getenv(\"OPENAI_BASE_URL\") # Loaded from .env for Crossover key\n",
    ")\n",
    "\n",
    "print(f\"Using LLM Model: {hermes_config.llm_model_name}\")\n",
    "if hermes_config.llm_base_url:\n",
    "    print(f\"Using LLM Base URL: {hermes_config.llm_base_url}\")\n",
    "print(f\"Input Spreadsheet ID: {INPUT_SPREADSHEET_ID}\")\n",
    "print(f\"Output Spreadsheet Name: {OUTPUT_SPREADSHEET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Function to Read Google Sheets\n",
    "This function reads data from a specified Google Sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_gsheet(document_id: str, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads a sheet from a Google Spreadsheet into a pandas DataFrame.\"\"\"\n",
    "    export_link = f\"https://docs.google.com/spreadsheets/d/{document_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "    try:\n",
    "        df = pd.read_csv(export_link)\n",
    "        # Standardize column names: lowercase and replace spaces with underscores\n",
    "        df.columns = [str(col).lower().replace(' ', '_') for col in df.columns]\n",
    "        print(f\"Successfully read {len(df)} rows from sheet: {sheet_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Google Sheet {sheet_name} from document {document_id}: {e}\")\n",
    "        if sheet_name == 'products':\n",
    "            return pd.DataFrame(columns=['product_id', 'name', 'category', 'stock_amount', 'description', 'price', 'season'])\n",
    "        elif sheet_name == 'emails':\n",
    "            return pd.DataFrame(columns=['email_id', 'subject', 'body'])\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Input Data\n",
    "Load the product catalog and emails from the input Google Spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Product Catalog\n",
    "print(f\"Loading product catalog from Google Sheet ID: {INPUT_SPREADSHEET_ID}, sheet: products\")\n",
    "product_catalog_df = read_data_from_gsheet(INPUT_SPREADSHEET_ID, 'products')\n",
    "if not product_catalog_df.empty:\n",
    "    display(product_catalog_df.head(3))\n",
    "else:\n",
    "    print(\"Failed to load product catalog. Using an empty DataFrame.\")\n",
    "\n",
    "# Load Emails Data\n",
    "print(f\"\nLoading emails from Google Sheet ID: {INPUT_SPREADSHEET_ID}, sheet: emails\")\n",
    "emails_df = read_data_from_gsheet(INPUT_SPREADSHEET_ID, 'emails')\n",
    "if not emails_df.empty:\n",
    "    # Ensure required columns exist\n",
    "    if 'email_id' not in emails_df.columns:\n",
    "        print(\"Error: 'email_id' column missing in emails sheet. Please check the input spreadsheet.\")\n",
    "        emails_df['email_id'] = None # Add dummy column to prevent errors later, though results will be affected\n",
    "    if 'subject' not in emails_df.columns:\n",
    "        emails_df['subject'] = \"\" # Add empty subject if missing\n",
    "    if 'body' not in emails_df.columns:\n",
    "        emails_df['body'] = \"\" # Add empty body if missing\n",
    "        \n",
    "    display(emails_df.head(3))\n",
    "else:\n",
    "    print(\"Failed to load emails. Using an empty list.\")\n",
    "\n",
    "# Prepare emails list for processing\n",
    "sample_emails_list = []\n",
    "if not emails_df.empty and 'email_id' in emails_df.columns:\n",
    "    # Use standardized column names from read_data_from_gsheet\n",
    "    column_mapping = {\n",
    "        'email_id': 'id',\n",
    "        'subject': 'subject',\n",
    "        'body': 'body'\n",
    "    }\n",
    "    emails_df_renamed = emails_df[list(column_mapping.keys())].rename(columns=column_mapping)\n",
    "    sample_emails_list = emails_df_renamed.to_dict(orient='records')\n",
    "    print(f\"\nPrepared {len(sample_emails_list)} emails for processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Emails with Analyzer Agent\n",
    "\n",
    "Iterate through the loaded emails, create an initial state for each, and call the `analyze_email_node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_email_analysis(emails_to_process: List[Dict[str, str]], \n",
    "                             p_catalog_df: pd.DataFrame, \n",
    "                             config_obj: HermesConfig) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyzes a list of emails using the analyze_email_node.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # The analyze_email_node needs a 'config' argument that's typically a RunnableConfig.\n",
    "    # We pass HermesConfig within the 'configurable' field as expected by agents.\n",
    "    runnable_config_for_node = {\n",
    "        \"configurable\": {\n",
    "            \"hermes_config\": config_obj\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for i, email_data in enumerate(emails_to_process):\n",
    "        print(f\"Processing email {i+1}/{len(emails_to_process)}: ID {email_data.get('id', 'N/A')}\")\n",
    "        \n",
    "        # Create initial state for the email analyzer node\n",
    "        # For this specific test, vector_store can be None as classifier primarily works on text.\n",
    "        # product_catalog_df might be useful if the analyzer has logic tied to it, but not strictly for classification.\n",
    "        initial_state_dict = HermesState(\n",
    "            email_id=str(email_data.get('id', f\"unknown_email_{i}\")), # Ensure email_id is a string\n",
    "            email_subject=email_data.get('subject', ''),\n",
    "            email_body=email_data.get('body', ''),\n",
    "            product_catalog_df=p_catalog_df, # Pass product catalog\n",
    "            vector_store=None # Not strictly needed for classifier alone\n",
    "        ).__dict__ # LangGraph nodes expect dictionaries\n",
    "        \n",
    "        try:\n",
    "            # Invoke the email analyzer node\n",
    "            analysis_output = await analyze_email_node(initial_state_dict, config=runnable_config_for_node)\n",
    "            \n",
    "            # The output is a dictionary, with 'email_analysis' key holding the EmailAnalysis model's dict form\n",
    "            if 'email_analysis' in analysis_output and analysis_output['email_analysis']:\n",
    "                # Reconstruct the Pydantic model for easier access and validation (optional but good practice)\n",
    "                email_analysis_result = EmailAnalysis(**analysis_output['email_analysis'])\n",
    "                results.append({\n",
    "                    \"email_id\": email_data.get('id'),\n",
    "                    \"classification\": email_analysis_result.classification.value, # Get enum's value\n",
    "                    \"classification_confidence\": email_analysis_result.classification_confidence,\n",
    "                    \"classification_evidence\": email_analysis_result.classification_evidence,\n",
    "                    \"reasoning\": email_analysis_result.reasoning,\n",
    "                    \"raw_analysis\": email_analysis_result.model_dump() # Store full analysis too\n",
    "                })\n",
    "                print(f\"  -> Classified as: {email_analysis_result.classification.value}\")\n",
    "            else:\n",
    "                print(f\"  -> Error: 'email_analysis' not found in node output or is empty.\")\n",
    "                results.append({\n",
    "                    \"email_id\": email_data.get('id'),\n",
    "                    \"classification\": \"error_no_analysis\",\n",
    "                    \"reasoning\": \"No analysis returned by the agent node.\",\n",
    "                    \"raw_analysis\": None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error processing email ID {email_data.get('id', 'N/A')}: {e}\")\n",
    "            results.append({\n",
    "                \"email_id\": email_data.get('id'),\n",
    "                \"classification\": \"error_exception\",\n",
    "                \"reasoning\": str(e),\n",
    "                \"raw_analysis\": None\n",
    "            })\n",
    "            # Optionally, re-raise if you want to stop on first error\n",
    "            # raise e \n",
    "            \n",
    "    return results\n",
    "\n",
    "# Run the analysis (this will execute the async function)\n",
    "if sample_emails_list: # Only run if emails were loaded\n",
    "    print(f\"\nStarting email analysis for {len(sample_emails_list)} emails...\")\n",
    "    processed_email_results = asyncio.run(\n",
    "        run_email_analysis(\n",
    "            emails_to_process=sample_emails_list,\n",
    "            p_catalog_df=product_catalog_df, # Pass the loaded product catalog\n",
    "            config_obj=hermes_config\n",
    "        )\n",
    "    )\n",
    "    print(\"\nEmail analysis complete.\")\n",
    "    \n",
    "    # Display first few results\n",
    "    if processed_email_results:\n",
    "        print(\"\nSample of Processed Results:\")\n",
    "        for res in processed_email_results[:3]:\n",
    "            print(f\"Email ID: {res['email_id']}, Classification: {res['classification']}, Confidence: {res.get('classification_confidence', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No emails to process.\")\n",
    "    processed_email_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Output Data for Google Sheets\n",
    "\n",
    "Format the results into a DataFrame matching the 'email-classification' sheet structure required by the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_classification_data = []\n",
    "if processed_email_results:\n",
    "    for result in processed_email_results:\n",
    "        email_classification_data.append({\n",
    "            \"email ID\": result.get(\"email_id\"),\n",
    "            \"category\": result.get(\"classification\", \"unknown\") # Ensure 'category' is the assignment col name\n",
    "        })\n",
    "\n",
    "email_classification_df = pd.DataFrame(email_classification_data)\n",
    "\n",
    "if not email_classification_df.empty:\n",
    "    print(\"\nEmail Classification DataFrame for Output:\")\n",
    "    display(email_classification_df.head())\n",
    "else:\n",
    "    print(\"No classification results to output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write Results to Google Sheets\n",
    "\n",
    "This section authenticates with Google, creates a new spreadsheet (or opens an existing one), and writes the `email_classification_df` to the 'email-classification' sheet. \n",
    "**Note:** This part requires being in a Google Colab environment or having local Google Cloud SDK authentication set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_and_get_gspread_client():\n",
    "    \"\"\"Authenticates and returns a gspread client. Works best in Colab.\"\"\"\n",
    "    try:\n",
    "        auth.authenticate_user() # Colab specific authentication\n",
    "        creds, _ = default()\n",
    "        gc = gspread.authorize(creds)\n",
    "        print(\"Successfully authenticated with Google Sheets.\")\n",
    "        return gc\n",
    "    except Exception as e:\n",
    "        print(f\"Google Sheets authentication failed: {e}\")\n",
    "        print(\"Please ensure you are running this in Google Colab or have local authentication configured.\")\n",
    "        return None\n",
    "\n",
    "def write_df_to_gsheet(gc: gspread.Client, spreadsheet_name: str, worksheet_name: str, df: pd.DataFrame, headers: List[str]):\n",
    "    \"\"\"Writes a DataFrame to a specified worksheet in a Google Spreadsheet.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"DataFrame for {worksheet_name} is empty. Nothing to write.\")\n",
    "        return\n",
    "    try:\n",
    "        # Try to open the spreadsheet, create if not found\n",
    "        try:\n",
    "            spreadsheet = gc.open(spreadsheet_name)\n",
    "            print(f\"Opened existing spreadsheet: '{spreadsheet_name}'\")\n",
    "        except gspread.exceptions.SpreadsheetNotFound:\n",
    "            print(f\"Spreadsheet '{spreadsheet_name}' not found. Creating new one...\")\n",
    "            spreadsheet = gc.create(spreadsheet_name)\n",
    "            print(f\"Created new spreadsheet: '{spreadsheet_name}'\")\n",
    "            # Share it so the user can see it easily\n",
    "            spreadsheet.share('', perm_type='anyone', role='reader') # Share publicly for easy access\n",
    "            print(f\"Publicly shared link: https://docs.google.com/spreadsheets/d/{spreadsheet.id}\")\n",
    "            \n",
    "        # Try to open the worksheet, create if not found\n",
    "        try:\n",
    "            worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "            print(f\"Opened existing worksheet: '{worksheet_name}'\")\n",
    "            worksheet.clear() # Clear existing data before writing new data\n",
    "            print(f\"Cleared existing data from worksheet: '{worksheet_name}'\")\n",
    "        except gspread.exceptions.WorksheetNotFound:\n",
    "            print(f\"Worksheet '{worksheet_name}' not found. Creating new one...\")\n",
    "            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=1, cols=len(headers))\n",
    "            print(f\"Created new worksheet: '{worksheet_name}'\")\n",
    "            \n",
    "        # Write headers\n",
    "        worksheet.update([headers], 'A1') # Explicitly set headers\n",
    "        # Write DataFrame content (excluding headers, starting from row 2)\n",
    "        set_with_dataframe(worksheet, df, row=2, include_column_header=False)\n",
    "        print(f\"Successfully wrote {len(df)} rows to worksheet '{worksheet_name}' in spreadsheet '{spreadsheet_name}'.\")\n",
    "        print(f\"Spreadsheet link: https://docs.google.com/spreadsheets/d/{spreadsheet.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to Google Sheet: {e}\")\n",
    "\n",
    "# Authenticate and write the results\n",
    "if not email_classification_df.empty:\n",
    "    gspread_client = authenticate_and_get_gspread_client()\n",
    "    if gspread_client:\n",
    "        classification_headers = [\"email ID\", \"category\"] # As per assignment\n",
    "        write_df_to_gsheet(\n",
    "            gc=gspread_client, \n",
    "            spreadsheet_name=OUTPUT_SPREADSHEET_NAME, \n",
    "            worksheet_name=\"email-classification\", \n",
    "            df=email_classification_df, \n",
    "            headers=classification_headers\n",
    "        )\n",
    "else:\n",
    "    print(\"No classification data to write to Google Sheets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- End of Notebook ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
 